{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6f6ad1e42e3504",
   "metadata": {},
   "source": [
    "# USW-AI-Projekt: Einfluss sozialer Medien auf Aktienkurse\n",
    "## CRISP-DM Struktur\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd09499a3af084",
   "metadata": {},
   "source": [
    "### 1. Business Understanding\n",
    "- Ziel: Zusammenhang zwischen Twitter-Stimmung und Aktienkursentwicklung untersuchen\n",
    "- Zielgruppe: Finanzanalysten, Anleger\n",
    "- Fragestellung: Kann man Kursbewegungen durch Stimmung erklären oder sogar vorhersagen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36898cf9824850b",
   "metadata": {},
   "source": [
    "### 2. Data Understanding\n",
    "- Datenquellen: Twitter API v2, yfinance\n",
    "- Zeitraum: z. B. letzte 3–6 Monate\n",
    "- Unternehmen: z. B. SAP, Siemens, BMW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9302d70a-8908-416a-8911-292f57ef4876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.63-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from yfinance) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from yfinance) (2.2.6)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from yfinance) (2.32.4)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Using cached frozendict-2.4.6-cp310-cp310-win_amd64.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.18.1-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from yfinance) (4.13.4)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Using cached curl_cffi-0.11.4-cp39-abi3-win_amd64.whl.metadata (14 kB)\n",
      "Collecting protobuf>=3.19.0 (from yfinance)\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Using cached websockets-15.0.1-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.0)\n",
      "Collecting cffi>=1.12.0 (from curl_cffi>=0.7->yfinance)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.6.15)\n",
      "Collecting pycparser (from cffi>=1.12.0->curl_cffi>=0.7->yfinance)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tugay\\usw-ai-app\\venv310\\lib\\site-packages (from requests>=2.31->yfinance) (2.5.0)\n",
      "Using cached yfinance-0.2.63-py2.py3-none-any.whl (118 kB)\n",
      "Using cached curl_cffi-0.11.4-cp39-abi3-win_amd64.whl (1.6 MB)\n",
      "Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Using cached frozendict-2.4.6-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Using cached websockets-15.0.1-cp310-cp310-win_amd64.whl (176 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: peewee, multitasking, websockets, pycparser, protobuf, frozendict, cffi, curl_cffi, yfinance\n",
      "\n",
      "   ---------------------------------------- 0/9 [peewee]\n",
      "   -------- ------------------------------- 2/9 [websockets]\n",
      "   -------- ------------------------------- 2/9 [websockets]\n",
      "   ------------- -------------------------- 3/9 [pycparser]\n",
      "   ----------------- ---------------------- 4/9 [protobuf]\n",
      "   ----------------- ---------------------- 4/9 [protobuf]\n",
      "   ---------------------- ----------------- 5/9 [frozendict]\n",
      "   ------------------------------- -------- 7/9 [curl_cffi]\n",
      "   ----------------------------------- ---- 8/9 [yfinance]\n",
      "   ----------------------------------- ---- 8/9 [yfinance]\n",
      "   ---------------------------------------- 9/9 [yfinance]\n",
      "\n",
      "Successfully installed cffi-1.17.1 curl_cffi-0.11.4 frozendict-2.4.6 multitasking-0.0.11 peewee-3.18.1 protobuf-6.31.1 pycparser-2.22 websockets-15.0.1 yfinance-0.2.63\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6495425a189b7add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:34:08.098314Z",
     "start_time": "2025-06-20T12:34:08.016762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "df = yf.download(\"SAP.DE\", start=\"2024-01-01\", end=\"2024-06-01\", auto_adjust=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4cc1f976cc309a",
   "metadata": {},
   "source": [
    "### 3. Data Preparation\n",
    "- Tweets säubern (Stopwörter, Emojis, etc.)\n",
    "- Sentimentanalyse: z. B. cardiffnlp/twitter-roberta-base-sentiment\n",
    "- Tagesweise aggregieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f47c6a63b96886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T13:29:27.879036Z",
     "start_time": "2025-06-20T13:29:27.809704Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\"))\n",
      "4 requests to https://twitter.com/search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click failed, giving up.\n",
      "Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)')))\"))\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAP lang:de since:2024-01-01 until:2024-06-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tweets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(query)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m300\u001b[39m:  \u001b[38;5;66;03m# Begrenze auf 300 Tweets (z. B. zum Testen)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\USW-AI-App\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\USW-AI-App\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32m~\\USW-AI-App\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:883\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_api_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint, apiType, params, instructionsPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 883\u001b[0m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_guest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    885\u001b[0m \t\tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n",
      "File \u001b[1;32m~\\USW-AI-App\\venv310\\lib\\site-packages\\snscrape\\modules\\twitter.py:825\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._ensure_guest_token\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guestTokenManager\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving guest token\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 825\u001b[0m \tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_baseUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_guest_token_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m (match \u001b[38;5;241m:=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.cookie = decodeURIComponent\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt=(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+); Max-Age=10800; Domain=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.twitter\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.com; Path=/; Secure\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m.\u001b[39mtext)):\n\u001b[0;32m    827\u001b[0m \t\t_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound guest token in HTML\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\USW-AI-App\\venv310\\lib\\site-packages\\snscrape\\base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\USW-AI-App\\venv310\\lib\\site-packages\\snscrape\\base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/search?f=live&lang=en&q=SAP+lang%3Ade+since%3A2024-01-01+until%3A2024-06-01&src=spelling_expansion_revert_click failed, giving up."
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "# Tweets mit dem Wort \"SAP\" seit 1. Januar 2024\n",
    "query = \"SAP lang:de since:2024-01-01 until:2024-06-01\"\n",
    "tweets = []\n",
    "\n",
    "for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "    if i > 300:  # Begrenze auf 300 Tweets (z. B. zum Testen)\n",
    "        break\n",
    "    tweets.append({\n",
    "        \"date\": tweet.date,\n",
    "        \"content\": tweet.content,\n",
    "        \"username\": tweet.user.username,\n",
    "        \"url\": tweet.url\n",
    "    })\n",
    "\n",
    "df_tweets = pd.DataFrame(tweets)\n",
    "df_tweets[\"date\"] = pd.to_datetime(df_tweets[\"date\"]).dt.date\n",
    "df_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50835b8f71fce85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platzhalter: Sentimentanalyse vorbereiten\n",
    "# z. B. Huggingface Transformers laden und Tweets analysieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6511cf8e04bf499",
   "metadata": {},
   "source": [
    "### 4. Modelling\n",
    "- Korrelation analysieren\n",
    "- Optional: Klassifikation ↑/↓ mittels Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ffe9031d2077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Pearson-Korrelation zwischen Stimmung & Kursveränderung berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b963d03674e5c63",
   "metadata": {},
   "source": [
    "### 5. Evaluation\n",
    "- Visualisierung mit matplotlib/seaborn/plotly\n",
    "- Streudiagramm, Zeitreihenvergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e77974748f005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Zeitreihe mit Kurs vs. Stimmung visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abddac8f555bd67",
   "metadata": {},
   "source": [
    "### 6. Ergebnis & Reporting\n",
    "- Interpretation der Ergebnisse\n",
    "- Empfehlung: Lohnt sich Social Media Analyse?\n",
    "- ggf. Streamlit-Dashboard oder PDF-Bericht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbfca409efcb8c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T13:29:21.041627Z",
     "start_time": "2025-06-20T13:29:20.955919Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileFinder' object has no attribute 'find_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnscrape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtwitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msntwitter\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msnscrape funktioniert!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\snscrape\\modules\\__init__.py:17\u001b[39m\n\u001b[32m     13\u001b[39m \t\tmodule = importer.find_module(moduleName).load_module(moduleName)\n\u001b[32m     14\u001b[39m \t\t\u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] = module\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m_import_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\snscrape\\modules\\__init__.py:13\u001b[39m, in \u001b[36m_import_modules\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m moduleNameWithoutPrefix = moduleName[prefixLen:]\n\u001b[32m     12\u001b[39m __all__.append(moduleNameWithoutPrefix)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m module = \u001b[43mimporter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_module\u001b[49m(moduleName).load_module(moduleName)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[moduleNameWithoutPrefix] = module\n",
      "\u001b[31mAttributeError\u001b[39m: 'FileFinder' object has no attribute 'find_module'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "print(\"snscrape funktioniert!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv310)",
   "language": "python",
   "name": "venv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
